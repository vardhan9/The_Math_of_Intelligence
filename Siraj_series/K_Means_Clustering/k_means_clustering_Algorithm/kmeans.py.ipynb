{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# K Means Clustering\n",
    "\n",
    "\n",
    "## Our Objective - Perform K-Means Clustering to detect Network Intrusion Attempts (Cybersecurity)\n",
    "\n",
    "![alt text](http://konukoii.com/blog/wp-content/uploads/2017/01/RunyanKmeans.gif \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "## What is K means Clustering??\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/cIDB3.png \"Logo Title Text 1\")\n",
    "\n",
    "What have we already learned?\n",
    "- The goal of machine learning is to optimize for an objective function. \n",
    "- There exist two optimization categories, first order and second order.\n",
    "- Both compute a loss function by minimizing the difference between a real label to a predicted label value through \n",
    "convex optimization (i.e gradient descent, newton's method)\n",
    "\n",
    "But what happens when we don't have labels for our data? \n",
    "\n",
    "### Enter unsupervised learning. \n",
    "\n",
    "![alt text](https://s-media-cache-ak0.pinimg.com/736x/8b/23/3e/8b233e2d7f26b00d0c594894917a127b--supervised-learning-variables.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/summit2014presentationfinal-141031070327-conversion-gate02/95/set-your-content-straight-28-638.jpg?cb=1414739431 \"Logo Title Text 1\")\n",
    "\n",
    "Clustering -  A typical and well-known type of unsupervised learning. Clustering algorithms try to find natural groupings in data. Similar data points (according to some notion of similarity) are considered in the same group. We call these groups clusters.\n",
    "\n",
    "K-Means clustering is a simple and widely-used clustering algorithm. Given value of  k , it tries to build  k  clusters from samples in the dataset. \n",
    "\n",
    "### How does it work ?\n",
    "\n",
    "![alt text](http://i.imgur.com/nIhdPrS.jpg \"Logo Title Text 1\")\n",
    "\n",
    "Given  k , the K-means algorithm works as follows:\n",
    "1. Randomly choose  k  data points (seeds) to be the initial centroids\n",
    "2. Assign each data point to the closest centroid\n",
    "3. Re-compute (update) the centroids using the current cluster memberships\n",
    "4. If a convergence criterion is not met, go to step 2\n",
    "\n",
    "We can also terminate the algorithm when it reaches an iteration budget, which yields an approximate result. The algorithm is sensitive to the order in which data samples are explored, so run it several times to get varied orders, then average the cluster centers from each run and input those centers as ones for one final run analysis.\n",
    "\n",
    "### How to determine the best value for k? \n",
    "\n",
    "Do you know how many classes you want to find? Use that as K. \n",
    "\n",
    "If not, The Elbow Method is a very popular way. \n",
    "\n",
    "![alt text](https://qph.ec.quoracdn.net/main-qimg-678795190794dd4c071366c06bf32115-c \"Logo Title Text 1\")\n",
    "![alt text](http://i.imgur.com/aLKDhbF.png \"Logo Title Text 1\")\n",
    "\n",
    "### How is the distance between centroids and data points measured?\n",
    "\n",
    "The Euclidean distance! Because K-means minimizes within-cluster variance and f you look at the definition of variance, it is identical to the sum of squared Euclidean distances from the center. Other distances are possible, euclidean is by far the most popular. \n",
    "\n",
    "## When should I use it?\n",
    "\n",
    "- Your data is numeric. It doesn't work with categorical features. We're computing the distance between real numbers!\n",
    "- If you don't have labels for your data\n",
    "- K-means is the simplest. To implement and to run. All you need to do is choose \"k\" and run it a number of times.\n",
    "- K-means and other clustering algorithms shine when you have multivariate data. They will \"work\" with 1-dimensional data, but they are not very smart anymore.\n",
    "- useful when you have an idea of how many clusters actually exists in your space. \n",
    "\n",
    "## Other examples\n",
    "\n",
    "- https://github.com/georgymh/ml-fraud-detection (Fraud Detection)\n",
    "- https://github.com/Datamine/MNIST-K-Means-Clustering/blob/master/Kmeans.ipynb (MNIST without labels (finally)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix math\n",
    "import numpy as np\n",
    "#graphing\n",
    "import matplotlib.pyplot as plt\n",
    "#graphing animation\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load textfile dataset (2D data points)\n",
    "# for each user, how many packets are sent per second and what's the size of a packet\n",
    "#anomalies (DDOS attempts) will have lots of big packets sent in a short amount of time \n",
    "def load_dataset(name):\n",
    "    return np.loadtxt(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/dc0281a964ec758cca02ab9ef91a7f54ac00d4b7 \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#euclidian distance between 2 data points. For as many data points as necessary. \n",
    "#np.linalg.norm(a-b) here a-b is a to b distance\n",
    "def euclidian(a, b):\n",
    "    return np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "O k-means algorithm \n",
    "With the above steps, since we needed to build our algorithm, which will receive as parameters:\n",
    "\n",
    "\n",
    "- K: The number of clusters (required)\n",
    "- epsilon: The minimum error to be used in the stop condition (optional, default == 0)\n",
    "- Distance: The method is used to calculate the distance (Optional defalut == 0)\n",
    "And has the return:\n",
    "- the centroids\n",
    "- The evolution history of centroids\n",
    "- And the membership vector of each instance with its respective centroid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(k, epsilon=0, distance='euclidian'):\n",
    "    #list to store past centroid\n",
    "    history_centroids = []\n",
    "    #set the distance calculation type \n",
    "    if distance == 'euclidian':\n",
    "        dist_method = euclidian\n",
    "    #set the dataset\n",
    "    dataset = load_dataset('durudataset.txt')\n",
    "    # dataset = dataset[:, 0:dataset.shape[1] - 1]\n",
    "    # get the number of rows (instances) and columns (features) from the dataset\n",
    "    num_instances, num_features = dataset.shape\n",
    "    #define k centroids (how many clusters do we want to find?) chosen randomly \n",
    "    prototypes = dataset[np.random.randint(0, num_instances - 1, size=k)]\n",
    "    #set these to our list of past centroid (to show progress over time)\n",
    "    history_centroids.append(prototypes)\n",
    "    #to keep track of centroid at every iteration\n",
    "    prototypes_old = np.zeros(prototypes.shape)\n",
    "    #to store clusters\n",
    "    belongs_to = np.zeros((num_instances, 1))\n",
    "    norm = dist_method(prototypes, prototypes_old)\n",
    "    iteration = 0\n",
    "    while norm > epsilon:\n",
    "        iteration += 1\n",
    "        norm = dist_method(prototypes, prototypes_old)\n",
    "        #for each instance in the dataset\n",
    "        for index_instance, instance in enumrate(dataset):\n",
    "            #define a distance vector of size k\n",
    "            dist_vec = np.zeros((k,1))\n",
    "            #for each centroid\n",
    "            for index_prototype, prototype in enumerate(prototypes):\n",
    "                #compute the distance between x and centroid\n",
    "                dist_vec[index_prototype] = dist_method(prototype, instance)\n",
    "            #find the smallest distance, assign that distance to a cluster\n",
    "            belongs_to[index_instance, 0] = np.argmin(dist_vec)\n",
    "            \n",
    "        tmp_prototypes = np.zeros((k, num_features))\n",
    "        \n",
    "        #for each cluster (k of them)\n",
    "        for index in range(len(prototypes)):\n",
    "            #get all the points assigned to a cluster\n",
    "            instances_close = [i for i in range(len(belongs_to)) if belongs_to[i] == index]\n",
    "            #find the mean of those points, this is our new centroid\n",
    "            prototype = np.mean(dataset[instances_close], axis=0)\n",
    "            #add our new centroid to our new temporary list\n",
    "            tmp_prototypes[index, :] = prototype\n",
    "        \n",
    "        #set the new list to the current list\n",
    "        prototypes = tmp_prototypes\n",
    "        \n",
    "        #add our calculated centroids to our history for plotting\n",
    "        history_centroids.append(tmp_prototypes)\n",
    "\n",
    "    #return calculated centroids, history of them all, and assignments for which cluster each datapoint belongs to\n",
    "    return prototypes, history_centroids, belongs_to\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets define a plotting algorithm for our dataset and our centroids\n",
    "def plot(dataset, history_centroids, belongs_to):\n",
    "    #we'll have 2 colors for each centroid cluster\n",
    "    colors = ['r', 'g']\n",
    "\n",
    "    #split our graph by its axis and actual plot\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    #for each point in our dataset\n",
    "    for index in range(dataset.shape[0]):\n",
    "        #get all the points assigned to a cluster\n",
    "        instances_close = [i for i in range(len(belongs_to)) if belongs_to[i] == index]\n",
    "        #assign each datapoint in that cluster a color and plot it\n",
    "        for instance_index in instances_close:\n",
    "            ax.plot(dataset[instance_index][0], dataset[instance_index][1], (colors[index] + 'o'))\n",
    "\n",
    "    #lets also log the history of centroids calculated via training\n",
    "    history_points = []\n",
    "    #for each centroid ever calculated\n",
    "    for index, centroids in enumerate(history_centroids):\n",
    "        #print them all out\n",
    "        for inner, item in enumerate(centroids):\n",
    "            if index == 0:\n",
    "                history_points.append(ax.plot(item[0], item[1], 'bo')[0])\n",
    "            else:\n",
    "                history_points[inner].set_data(item[0], item[1])\n",
    "                print(\"centroids {} {}\".format(index, item))\n",
    "\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main file \n",
    "def execute():\n",
    "    #load dataset\n",
    "    dataset = load_dataset('durudataset.txt')\n",
    "    #train the model on the data\n",
    "    centroids, history_centroids, belongs_to = kmeans(2)\n",
    "    #plot the results\n",
    "    plot(dataset, history_centroids, belongs_to)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enumrate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-565a1ad53272>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#do everything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-1b7a77a82868>\u001b[0m in \u001b[0;36mexecute\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'durudataset.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#train the model on the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcentroids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_centroids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbelongs_to\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#plot the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_centroids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbelongs_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c64b00bf7adc>\u001b[0m in \u001b[0;36mkmeans\u001b[0;34m(k, epsilon, distance)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprototypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprototypes_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#for each instance in the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mindex_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;31m#define a distance vector of size k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mdist_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enumrate' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "#do everything\n",
    "execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "def plot_step_by_step(dataset, history_centroids, belongs_to):\n",
    "    colors = ['r', 'g']\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for index in range(dataset.shape[0]):\n",
    "        instances_close = [i for i in range(len(belongs_to)) if belongs_to[i] == index]\n",
    "        for instance_index in instances_close:\n",
    "            ax.plot(dataset[instance_index][0], dataset[instance_index][1], (colors[index] + 'o'))\n",
    "\n",
    "    history_points = []\n",
    "    for index, centroids in enumerate(history_centroids):\n",
    "        for inner, item in enumerate(centroids):\n",
    "            if index == 0:\n",
    "                history_points.append(ax.plot(item[0], item[1], 'bo')[0])\n",
    "            else:\n",
    "                history_points[inner].set_data(item[0], item[1])\n",
    "                print(\"centroids {} {}\".format(index, item))\n",
    "                \n",
    "                plt.pause(0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_centroids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-74ee5a7aa776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory_centroids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mplot_step_by_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbelongs_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history_centroids' is not defined"
     ]
    }
   ],
   "source": [
    "for item in history_centroids:\n",
    "    plot_step_by_step(dataset, [item], belongs_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
